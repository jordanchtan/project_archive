{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7 Express: CNN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPzpAn3M+Jf9DjFwJbd9sje"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"c8B9sNNcwyBq","colab_type":"code","outputId":"5c3171ee-ebba-4aa7-e3ab-73f881a89a57","executionInfo":{"status":"ok","timestamp":1591718146239,"user_tz":-480,"elapsed":2141,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x_wbMW_7mmNu","colab_type":"code","colab":{}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzN2-yCYkQnq","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"1-VbNYCEjJvo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","  # load your data using this function\n","  # url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ExpressData/2_No_Likes.csv'\n","  url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ExpressDataPre/EmoBank_Writer_All.csv'\n","  df = pd.read_csv(url, encoding='utf8')\n","  df = df[df['text'].apply(lambda x: isinstance(x, str))]\n","  \n","  data = df['text']\n","  labels = df[['V','A','D']]\n","\n","  data = data.values\n","  labels = labels.values\n","\n","  return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiG3Ed4kW02","colab_type":"text"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"D6Z_2bSpkcrF","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, Embedding, Dropout\n","from keras.utils import plot_model\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.layers import Embedding\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","\n","metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","# max_features = 5000\n","# maxlen = 400\n","# batch_size = 32\n","# embedding_dims = 50\n","filters = 250\n","kernel_size = 3\n","hidden_dims = 250\n","# epochs = 2\n","\n","def create_model(embedding_layer):\n","  model = Sequential()\n","  model.add(embedding_layer)\n","  model.add(Dropout(0.2))\n","\n","  # we add a Convolution1D, which will learn filters\n","  # word group filters of size filter_length:\n","  model.add(Conv1D(filters,\n","                  kernel_size,\n","                  padding='valid',\n","                  activation='relu',\n","                  strides=1))\n","  # we use max pooling:\n","  model.add(GlobalMaxPooling1D())\n","\n","  # We add a vanilla hidden layer:\n","  model.add(Dense(hidden_dims))\n","  model.add(Dropout(0.2))\n","  model.add(Activation('relu'))\n","\n","  # model.add(Flatten())\n","  model.add(Dense(units=3, activation='relu'))\n","  \n","  # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","  # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","  model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","  print(model.summary())\n","  from keras.utils.vis_utils import plot_model\n","  plot_model(model, to_file='7_Express_CNN.png', show_shapes=True, show_layer_names=True)\n","\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAGxM4t4kdFc","colab_type":"text"},"source":["# Train and Evaluate Model"]},{"cell_type":"code","metadata":{"id":"_RoJspSWkhhL","colab_type":"code","outputId":"7a127544-047e-4247-d5a1-be8af6f5cfe2","executionInfo":{"status":"ok","timestamp":1591718146243,"user_tz":-480,"elapsed":2119,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test):\n","  print(\"Training:\")\n","  # data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2, shuffle=True)\n","  data_test, data_val, labels_test, labels_val = train_test_split(data_test, labels_test, test_size=0.5, shuffle=True)\n","\n","  model.fit(data_train, labels_train, \n","        epochs=15, batch_size=128, verbose=1, shuffle=True,\n","        validation_data=(data_val, labels_val))\n","  \n","  print(\"Evaluating:\")\n","  scores = model.evaluate(data_test, labels_test, verbose=1)\n","  print(\"Final scores for fold:\")\n","  print(model.metrics_names, scores) \n","  return scores"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2G1rqoZ-lmIY","colab_type":"text"},"source":["# Run Evaluation"]},{"cell_type":"code","metadata":{"id":"pVL64lSAZ1KW","colab_type":"code","outputId":"810f537c-c6d7-4e59-bf31-8ae3fe8530fc","executionInfo":{"status":"ok","timestamp":1591718147657,"user_tz":-480,"elapsed":3526,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["data, labels = load_data()\n","print(len(data))\n","print(data[0])\n","useHoldout = False\n","\n","min_reacts = 1\n","if (len(data) > 10000):\n","  useHoldout = True"],"execution_count":13,"outputs":[{"output_type":"stream","text":["10277\n","today i kept it simple .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gyklDbWwzkUS","colab_type":"text"},"source":["Prep embeddings"]},{"cell_type":"code","metadata":{"id":"I8wEi4vnmlB6","colab_type":"code","outputId":"8a153e97-d214-4b27-d495-ac2b4c2518ff","executionInfo":{"status":"ok","timestamp":1591718200491,"user_tz":-480,"elapsed":56353,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from gensim.models.keyedvectors import KeyedVectors\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding\n","\n","# path = './drive/My Drive/Colab Notebooks/Store/GoogleNews-vectors-negative300.bin.gz'\n","path = './drive/My Drive/Colab Notebooks/Store/ewe_uni_w2v_model.txt'\n","# TOP_K = 20000\n","embedding_dim = 300\n","t = Tokenizer()\n","t.fit_on_texts(data)\n","vocab_size = len(t.word_index) + 1\n","# integer encode the documents\n","encoded_data = t.texts_to_sequences(data)\n","max_length = len(max(encoded_data, key=len))\n","# MAX_SEQUENCE_LENGTH = 500\n","# if max_length > MAX_SEQUENCE_LENGTH:\n","#     max_length = MAX_SEQUENCE_LENGTH\n","padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n","data = padded_data\n","\n","# MAX_NB_WORDS = 200000\n","word2vec = KeyedVectors.load_word2vec_format(path, binary=False)\n","\n","word_index = t.word_index\n","nb_words = min(len(word_index), len(word_index))+1\n","# nb_words = min(MAX_NB_WORDS, len(word_index))+1\n","\n","embedding_matrix = np.zeros((nb_words, embedding_dim))\n","for word, i in word_index.items():\n","    if word in word2vec.vocab:\n","        embedding_matrix[i] = word2vec.word_vec(word)\n","# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n","\n","embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n","                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=max_length,\n","                            trainable=False)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NU6yr2dd5N6a","colab_type":"code","outputId":"182a003a-e36d-4c8a-92bc-b9b439456440","executionInfo":{"status":"ok","timestamp":1591718200493,"user_tz":-480,"elapsed":56346,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(data.shape)\n","print(max_length)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(10277, 124)\n","124\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-ZN6gj0qTR4","colab_type":"text"},"source":["K-Fold"]},{"cell_type":"code","metadata":{"id":"jb4ElLALlpDB","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import normalize\n","\n","\n","if not useHoldout:\n","  print(\"KFOLD\")  \n","  n_folds = 5\n","  kf = KFold(n_folds, shuffle=True)\n","  i = 0\n","\n","  # Define per-fold score containers\n","  scores_per_fold = []\n","\n","  for train_index, test_index in kf.split(data):\n","    print(\"Running Fold\", i+1, \"/\", n_folds)\n","    data_train, data_test = data[train_index], data[test_index]\n","    labels_train, labels_test = labels[train_index], labels[test_index]\n","    \n","    model = None # Clearing the NN.\n","    model = create_model(embedding_layer)\n","\n","    # scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","    # scores_per_fold.append(scores)\n","\n","    i += 1\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fFIvAn4WA02","colab_type":"code","colab":{}},"source":["if not useHoldout:\n","\n","  print('Average scores across all folds:')\n","  for metric_index, metric_name in enumerate(metrics):\n","    metric_total = 0\n","    for scores in scores_per_fold:\n","      metric_total += scores[metric_index + 1]\n","    print(metric_name, metric_total/n_folds )\n","  print(scores_per_fold)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8BF47pGqVpa","colab_type":"text"},"source":["Holdout"]},{"cell_type":"code","metadata":{"id":"UTR6zCWejJpJ","colab_type":"code","outputId":"60ddf785-a8f3-4a2a-ca1d-3b6715f99e7c","executionInfo":{"status":"ok","timestamp":1591718201447,"user_tz":-480,"elapsed":57279,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["from sklearn.model_selection import train_test_split\n","\n","if useHoldout:\n","  print(\"HOLDOUT\")\n","\n","  data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, shuffle=True)\n","\n","  model = None # Clearing the NN.\n","  model = create_model(embedding_layer)\n","\n","  # scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","  # print(model.metrics_names, scores) "],"execution_count":18,"outputs":[{"output_type":"stream","text":["HOLDOUT\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 124, 300)          4896300   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 124, 300)          0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 122, 250)          225250    \n","_________________________________________________________________\n","global_max_pooling1d_1 (Glob (None, 250)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 250)               62750     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 250)               0         \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 250)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 753       \n","=================================================================\n","Total params: 5,185,053\n","Trainable params: 288,753\n","Non-trainable params: 4,896,300\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZmOyBizEqwB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"3cec9d2a-5833-41f9-cbfe-338c11bab404","executionInfo":{"status":"ok","timestamp":1591718201448,"user_tz":-480,"elapsed":57272,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}}},"source":["# data, labels = load_data()\n","example_data = ['fuck you', 'i love you']\n","print(example_data)\n","encoded_example_data = t.texts_to_sequences(example_data)\n","padded_example_data = pad_sequences(encoded_example_data, maxlen=max_length, padding='post')\n","example_data = padded_example_data\n","print(model.predict(example_data, verbose=0))\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["['fuck you', 'i love you']\n","[[0.12897786 0.05301623 0.12357306]\n"," [0.32614148 0.12627119 0.05597237]]\n"],"name":"stdout"}]}]}