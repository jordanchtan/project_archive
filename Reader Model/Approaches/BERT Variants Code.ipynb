{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"albert.ipynb","provenance":[{"file_id":"1BkkCF6FXkcE_hb5S2Le2uZ2T9yFXE2Y8","timestamp":1585782213261}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b4c4e272fcf741a6829d642eaf020d7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8133bd9a4f0041fbaf1b55e17c0e7d31","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b6a5feb4268f4654818f5ba679221b8f","IPY_MODEL_689e4109036b484982027f03ff4c6ed7"]}},"8133bd9a4f0041fbaf1b55e17c0e7d31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6a5feb4268f4654818f5ba679221b8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a0b23123dc844d58025e09cb2b8892c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d7cfd69dd8c4472a373be17401c713f"}},"689e4109036b484982027f03ff4c6ed7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8638c71a5f5443d79588db7f1a4a65af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 2.54kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_514a7106bc524422ac2448857831ed57"}},"7a0b23123dc844d58025e09cb2b8892c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9d7cfd69dd8c4472a373be17401c713f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8638c71a5f5443d79588db7f1a4a65af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"514a7106bc524422ac2448857831ed57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"859c28fca06c4620979e9e9aecb93681":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0461220cd91459f99a42511d79c69ad","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_afac687ff559413da50cf16e0417339a","IPY_MODEL_66fc55b12b4c4da39c60fcdf1b83133e"]}},"d0461220cd91459f99a42511d79c69ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"afac687ff559413da50cf16e0417339a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8fcfe4e76b1e49cfb2e7aa44ae4a12c2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d35bd3da8c96460fa86ede18f4acdde3"}},"66fc55b12b4c4da39c60fcdf1b83133e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_162cf190497f477fa89a4d99cbe0366c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:05&lt;00:00, 44.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_137ab35fee064728b13e2f2abd5158a4"}},"8fcfe4e76b1e49cfb2e7aa44ae4a12c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d35bd3da8c96460fa86ede18f4acdde3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"162cf190497f477fa89a4d99cbe0366c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"137ab35fee064728b13e2f2abd5158a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"998949525eaf444e8d74bb2874cc963f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1ddacc2664734aeca818144ce577e269","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0d2b95b2bb6465882055eee160f7b9d","IPY_MODEL_66b6e2548e214fcea077f5da8ea78f30"]}},"1ddacc2664734aeca818144ce577e269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0d2b95b2bb6465882055eee160f7b9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e1c9bf04b07a44aeba041599c7ea8052","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fae5a64eadae4920b734a78a2a3fbe73"}},"66b6e2548e214fcea077f5da8ea78f30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dd17496de46448ae9e8b075757353f74","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 696kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c0e229e686f5447aa039c56d69dff698"}},"e1c9bf04b07a44aeba041599c7ea8052":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fae5a64eadae4920b734a78a2a3fbe73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd17496de46448ae9e8b075757353f74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c0e229e686f5447aa039c56d69dff698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3733f527e0804d70beb7172618b53324":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_56541ae2361e45a6b04fb76c836a8607","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a56a176c6bbd4641bab361b823929976","IPY_MODEL_89f9b0f6566d458380604a2dd33656bd"]}},"56541ae2361e45a6b04fb76c836a8607":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a56a176c6bbd4641bab361b823929976":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_54dea558dc434285b3b95142d069a4cc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_24901bdce1804698bdb0f0e520c9aafb"}},"89f9b0f6566d458380604a2dd33656bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fe2d03777c6b44b29cb3ec8e9278ef0d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 499B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9266a1ebe9ed446c95387cd040b13e1f"}},"54dea558dc434285b3b95142d069a4cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"24901bdce1804698bdb0f0e520c9aafb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe2d03777c6b44b29cb3ec8e9278ef0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9266a1ebe9ed446c95387cd040b13e1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40d8b03f35c546128ba7b03c66084ac9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c16c07de66e44f9789553aeefebe323b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c10b598e59ba403880d58e7475fde4fe","IPY_MODEL_a7a1a28b7ff747cdb24899c786a22406"]}},"c16c07de66e44f9789553aeefebe323b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c10b598e59ba403880d58e7475fde4fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5348a454aa9d46a9a735c933264aee57","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_94b54000a342467c80365fbdc1aaefd8"}},"a7a1a28b7ff747cdb24899c786a22406":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_97d0d849767b4b8ba6be567984800978","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [03:30&lt;00:00, 2.09MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca27def80c9f4f45a9707e6279800263"}},"5348a454aa9d46a9a735c933264aee57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"94b54000a342467c80365fbdc1aaefd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"97d0d849767b4b8ba6be567984800978":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca27def80c9f4f45a9707e6279800263":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"pGeO-wWrWEc6","colab_type":"code","outputId":"c88e2890-c5d7-4d5a-9c56-3e480fbfd42d","executionInfo":{"status":"ok","timestamp":1591777412975,"user_tz":-480,"elapsed":27067,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SK4QI01qmUkH","outputId":"f78842f7-b395-49c9-e93e-f601d89325a6","executionInfo":{"status":"ok","timestamp":1591777422412,"user_tz":-480,"elapsed":36490,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":608}},"source":["pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n","\r\u001b[K     |▌                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 5.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 6.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\r\u001b[K     |▎                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 29.4MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 32.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 33.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 36.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 32.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 34.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 35.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 32.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 32.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 32.6MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 32.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 32.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 32.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 32.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 32.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 32.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 32.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 32.6MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 40.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=48d9ab7dc48220fc62a2045452a2addcafb8d05acedb6006aaf4fcec0105dba2\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JusPEFvnwv2-","colab_type":"code","colab":{}},"source":["from transformers import DistilBertPreTrainedModel, DistilBertModel\n","import torch.nn as nn\n","\n","class MyDistilBertForSequenceClassification(DistilBertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, 3)\n","        # self.classifier = nn.Linear(config.dim, config.num_labels)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n","        \n","        distilbert_output = self.distilbert(\n","            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n","        )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","        # new addition jordan\n","        # logits = nn.ReLU()(logits)\n","        outputs = (logits,) + distilbert_output[1:]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1, 3), labels.view(-1, 3))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9PmoB9bnUkN","colab_type":"code","colab":{}},"source":["# from transformers import AlbertPreTrainedModel, AlbertModel\n","# import torch.nn as nn\n","# class MyAlbertForSequenceClassification(AlbertPreTrainedModel):\n","#     def __init__(self, config):\n","#         super().__init__(config)\n","#         self.num_labels = config.num_labels\n","\n","#         self.albert = AlbertModel(config)\n","#         self.dropout = nn.Dropout(config.classifier_dropout_prob)\n","#         self.classifier = nn.Linear(config.hidden_size, 3)\n","#         self.init_weights()\n","\n","#     def forward(\n","#         self,\n","#         input_ids=None,\n","#         attention_mask=None,\n","#         token_type_ids=None,\n","#         position_ids=None,\n","#         head_mask=None,\n","#         inputs_embeds=None,\n","#         labels=None,\n","#     ):\n","\n","#         outputs = self.albert(\n","#             input_ids=input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids,\n","#             position_ids=position_ids,\n","#             head_mask=head_mask,\n","#             inputs_embeds=inputs_embeds,\n","#         )\n","\n","#         pooled_output = outputs[1]\n","\n","#         pooled_output = self.dropout(pooled_output)\n","#         logits = self.classifier(pooled_output)\n","\n","#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","#         if labels is not None:\n","#             if self.num_labels == 1:\n","#                 loss_fct = nn.MSELoss()\n","#                 # loss_fct = nn.CrossEntropyLoss()\n","#                 loss = loss_fct(logits.view(-1, 3), labels.view(-1, 3))\n","#             else:\n","#                 loss_fct = nn.CrossEntropyLoss()\n","#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","#             outputs = (loss,) + outputs\n","\n","#         return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SDiye630qBBe","colab_type":"code","colab":{}},"source":["# from transformers import BertPreTrainedModel, BertModel\n","\n","# class MyBertForSequenceClassification(BertPreTrainedModel):\n","#     def __init__(self, config):\n","#         super().__init__(config)\n","#         self.num_labels = config.num_labels\n","\n","#         self.bert = BertModel(config)\n","#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","#         self.classifier = nn.Linear(config.hidden_size, 5)\n","\n","#         self.init_weights()\n","\n","#     def forward(\n","#         self,\n","#         input_ids=None,\n","#         attention_mask=None,\n","#         token_type_ids=None,\n","#         position_ids=None,\n","#         head_mask=None,\n","#         inputs_embeds=None,\n","#         labels=None,\n","#     ):\n","        \n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids,\n","#             position_ids=position_ids,\n","#             head_mask=head_mask,\n","#             inputs_embeds=inputs_embeds,\n","#         )\n","\n","#         pooled_output = outputs[1]\n","\n","#         pooled_output = self.dropout(pooled_output)\n","#         logits = self.classifier(pooled_output)\n","\n","#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","#         if labels is not None:\n","#             if self.num_labels == 1:\n","#                 loss_fct = nn.MSELoss()\n","#                 loss = loss_fct(logits.view(-1, 5), labels.view(-1, 5))\n","#             else:\n","#                 loss_fct = nn.CrossEntropyLoss()\n","#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","#             outputs = (loss,) + outputs\n","\n","#         return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2YYKxnIt4CF","colab_type":"code","colab":{}},"source":["# from transformers import BertPreTrainedModel, BertModel, RobertaConfig, RobertaModel\n","# class MyRobertaClassificationHead(nn.Module):\n","#     \"\"\"Head for sentence-level classification tasks.\"\"\"\n","\n","#     def __init__(self, config):\n","#         super().__init__()\n","#         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","#         self.out_proj = nn.Linear(config.hidden_size, 3)\n","\n","#     def forward(self, features, **kwargs):\n","#         x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n","#         x = self.dropout(x)\n","#         x = self.dense(x)\n","#         x = torch.tanh(x)\n","#         x = self.dropout(x)\n","#         x = self.out_proj(x)\n","#         return x\n","\n","# class MyRobertaForSequenceClassification(BertPreTrainedModel):\n","#     config_class = RobertaConfig\n","#     base_model_prefix = \"roberta\"\n","\n","#     def __init__(self, config):\n","#         super().__init__(config)\n","#         self.num_labels = config.num_labels\n","\n","#         self.roberta = RobertaModel(config)\n","#         self.classifier = MyRobertaClassificationHead(config)\n","\n","#     def forward(\n","#         self,\n","#         input_ids=None,\n","#         attention_mask=None,\n","#         token_type_ids=None,\n","#         position_ids=None,\n","#         head_mask=None,\n","#         inputs_embeds=None,\n","#         labels=None,\n","#     ):\n","#         outputs = self.roberta(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids,\n","#             position_ids=position_ids,\n","#             head_mask=head_mask,\n","#             inputs_embeds=inputs_embeds,\n","#         )\n","#         sequence_output = outputs[0]\n","#         logits = self.classifier(sequence_output)\n","\n","#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","#         if labels is not None:\n","#             if self.num_labels == 1:\n","#                 loss_fct = nn.MSELoss()\n","#                 loss = loss_fct(logits.view(-1, 3), labels.view(-1, 3))\n","#             else:\n","#                 loss_fct = nn.CrossEntropyLoss()\n","#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","#             outputs = (loss,) + outputs\n","\n","#         return outputs  # (loss), logits, (hidden_states), (attentions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdYMzuJt-30E","colab_type":"code","outputId":"b59bb036-6202-42c3-e550-4b40b4c04f47","executionInfo":{"status":"ok","timestamp":1591777655192,"user_tz":-480,"elapsed":10996,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b4c4e272fcf741a6829d642eaf020d7d","8133bd9a4f0041fbaf1b55e17c0e7d31","b6a5feb4268f4654818f5ba679221b8f","689e4109036b484982027f03ff4c6ed7","7a0b23123dc844d58025e09cb2b8892c","9d7cfd69dd8c4472a373be17401c713f","8638c71a5f5443d79588db7f1a4a65af","514a7106bc524422ac2448857831ed57","859c28fca06c4620979e9e9aecb93681","d0461220cd91459f99a42511d79c69ad","afac687ff559413da50cf16e0417339a","66fc55b12b4c4da39c60fcdf1b83133e","8fcfe4e76b1e49cfb2e7aa44ae4a12c2","d35bd3da8c96460fa86ede18f4acdde3","162cf190497f477fa89a4d99cbe0366c","137ab35fee064728b13e2f2abd5158a4"]}},"source":["from transformers import AlbertForSequenceClassification, AdamW, BertConfig, DistilBertForSequenceClassification\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = MyDistilBertForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4c4e272fcf741a6829d642eaf020d7d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"859c28fca06c4620979e9e9aecb93681","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MyDistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"39QHg5jEm-1E","outputId":"88cb150f-c4b7-4dbe-958a-9186ee1f4183","executionInfo":{"status":"ok","timestamp":1591771348981,"user_tz":-480,"elapsed":36088,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PNn8UJmGi8z6","colab_type":"text"},"source":["# Data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EvfAgixqAOpQ","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactData/2_No_Likes.csv'\n","# url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactData/8_No_Likes_min_100.csv'\n","# url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCountsPre/2_No_Likes.csv'\n","df = pd.read_csv(url, encoding='utf16')\n","\n","n = 1\n","df = df.head(int(len(df)*(n/100)))\n","\n","data = df['name']\n","\n","data = data.values\n","labels = df.select_dtypes(include=[np.number])\n","\n","labels = labels.values\n","# labels = labels/labels.sum(axis=1, keepdims=True)\n","\n","sentences, sentences_holdout, labels, labels_holdout = train_test_split(data, labels, test_size=0.001, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Nq5cbrQf9pg","colab_type":"code","outputId":"faac1806-b2f1-40a2-a09f-9b91d9dbc8c2","executionInfo":{"status":"ok","timestamp":1591771352512,"user_tz":-480,"elapsed":39596,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["print(sentences[0])\n","print(labels[0])\n","print(sentences.shape)\n","print(labels.shape)\n","print(sentences_holdout[:3])\n","print(labels_holdout[:3])\n","print(sentences_holdout.shape)\n","print(labels_holdout.shape)\n","\n","# np.save('./sentences_holdout.npy', sentences_holdout,allow_pickle=True)\n","# np.save('./labels_holdout.npy', labels_holdout,allow_pickle=True)\n","\n","# !cp sentences_holdout.npy \"./drive/My Drive/Colab Notebooks/MainModels\"\n","# !cp labels_holdout.npy \"./drive/My Drive/Colab Notebooks/MainModels\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Woman Makes 29-Gallon Breast Milk Donation\n","[1. 0. 0. 0. 0.]\n","(1554,)\n","(1554, 5)\n","['How bad are most of India’s medical schools? Very, according to new reports.'\n"," \"Ivanka Trump's brand will survive election no matter who wins, experts say\"]\n","[[0.02857143 0.51428571 0.05714286 0.28571429 0.11428571]\n"," [0.12173913 0.05217391 0.32173913 0.02608696 0.47826087]]\n","(2,)\n","(2, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3MSi8yOnLgoB","outputId":"1eb29f1a-b30c-4750-a06e-533931b3f5ef","executionInfo":{"status":"ok","timestamp":1591771535071,"user_tz":-480,"elapsed":2075,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["998949525eaf444e8d74bb2874cc963f","1ddacc2664734aeca818144ce577e269","e0d2b95b2bb6465882055eee160f7b9d","66b6e2548e214fcea077f5da8ea78f30","e1c9bf04b07a44aeba041599c7ea8052","fae5a64eadae4920b734a78a2a3fbe73","dd17496de46448ae9e8b075757353f74","c0e229e686f5447aa039c56d69dff698"]}},"source":["from transformers import BertTokenizer, DistilBertTokenizer, RobertaTokenizer,AlbertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","# or cased?\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"998949525eaf444e8d74bb2874cc963f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MK0DymCJMRxe","outputId":"7d2808f0-cae1-4cea-a29f-ff0b3a1abb98","executionInfo":{"status":"ok","timestamp":1591771355299,"user_tz":-480,"elapsed":42353,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["max_len = 0\n","# For every sentence...\n","for sent in sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","for sent in sentences_holdout:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Max sentence length:  35\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eju8ZcpePXNS","outputId":"9fe84a63-03ba-497a-b7df-1e160ced0d4c","executionInfo":{"status":"ok","timestamp":1591771356754,"user_tz":-480,"elapsed":43789,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    \n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = \"pt\",     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Original:  Woman Makes 29-Gallon Breast Milk Donation\n","Token IDs: tensor([    0,   693,   817,  1132,    12, 30322,   261,  6181,  5803,  7096,\n","            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R6qmOo1_Rlkd","outputId":"93dd226a-a454-4765-a0e5-ebfa62b5940e","executionInfo":{"status":"ok","timestamp":1591771356754,"user_tz":-480,"elapsed":43777,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.90 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1,398 training samples\n","  156 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kKV5HYV_KyNd","colab":{}},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vi-MNluOLExU","outputId":"d9496020-6fe4-496b-9f79-78f744731cdd","executionInfo":{"status":"ok","timestamp":1591771563720,"user_tz":-480,"elapsed":23944,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3733f527e0804d70beb7172618b53324","56541ae2361e45a6b04fb76c836a8607","a56a176c6bbd4641bab361b823929976","89f9b0f6566d458380604a2dd33656bd","54dea558dc434285b3b95142d069a4cc","24901bdce1804698bdb0f0e520c9aafb","fe2d03777c6b44b29cb3ec8e9278ef0d","9266a1ebe9ed446c95387cd040b13e1f","40d8b03f35c546128ba7b03c66084ac9","c16c07de66e44f9789553aeefebe323b","c10b598e59ba403880d58e7475fde4fe","a7a1a28b7ff747cdb24899c786a22406","5348a454aa9d46a9a735c933264aee57","94b54000a342467c80365fbdc1aaefd8","97d0d849767b4b8ba6be567984800978","ca27def80c9f4f45a9707e6279800263"]}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig, DistilBertForSequenceClassification\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = MyBertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3733f527e0804d70beb7172618b53324","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40d8b03f35c546128ba7b03c66084ac9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MyBertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"82JF5TTKMC3r","outputId":"4a664a36-fe19-47c7-ee1e-6cacebdc438d","executionInfo":{"status":"ok","timestamp":1591517118657,"user_tz":-480,"elapsed":13276,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":605}},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The BERT model has 203 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","roberta.embeddings.word_embeddings.weight               (50265, 768)\n","roberta.embeddings.position_embeddings.weight             (514, 768)\n","roberta.embeddings.token_type_embeddings.weight             (1, 768)\n","roberta.embeddings.LayerNorm.weight                           (768,)\n","roberta.embeddings.LayerNorm.bias                             (768,)\n","\n","==== First Transformer ====\n","\n","roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.query.bias             (768,)\n","roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n","roberta.encoder.layer.0.attention.self.key.bias               (768,)\n","roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.value.bias             (768,)\n","roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n","roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n","roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n","roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n","roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n","roberta.encoder.layer.0.output.dense.bias                     (768,)\n","roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n","roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n","\n","==== Output Layer ====\n","\n","classifier.dense.weight                                   (768, 768)\n","classifier.dense.bias                                         (768,)\n","classifier.out_proj.weight                                  (5, 768)\n","classifier.out_proj.bias                                        (5,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ILDW4fDpMMgD","colab":{}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hAjC5OxMMkAc","colab":{}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 2\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZtQAkAjwQZvu","colab":{}},"source":["import numpy as np\n","import sklearn\n","\n","def flat_mse(preds, labels):\n","    pred_flat = preds.flatten()\n","    labels_flat = labels.flatten()\n","    # result = (np.square(preds - labels)).mean(axis=None)\n","    result = sklearn.metrics.mean_squared_error(labels, preds)\n","    return result\n","\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","\n","from scipy.spatial.distance import jensenshannon\n","\n","def my_js_dist(list_p, list_q):\n","  list_p = np.abs(np.concatenate(list_p))\n","  list_q = np.abs(np.concatenate(list_q))\n","\n","  result = 0\n","  for index, p in enumerate(list_p):\n","    q = list_q[index]\n","    result += jensenshannon(p, q, base=2)\n","  \n","  return result / len(list_p)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioZeJDKyjbn7","colab_type":"text"},"source":["# Training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aMqnCx4PQgLV","outputId":"31cd77e2-9fc3-4476-b205-93e7677130ce","executionInfo":{"status":"ok","timestamp":1591517139641,"user_tz":-480,"elapsed":34222,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":571}},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","model.double()\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                            #  token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions , true_labels = [], []\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                  #  token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        acc = flat_mse(logits, label_ids)\n","        total_eval_accuracy += acc\n","\n","        # Store predictions and true labels\n","        predictions.append(logits)\n","        true_labels.append(label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    avg_val_mse = sklearn.metrics.mean_squared_error(np.concatenate(true_labels).ravel(), np.concatenate(predictions).ravel())\n","    avg_val_mae = sklearn.metrics.mean_absolute_error(np.concatenate(true_labels).ravel(), np.concatenate(predictions).ravel())\n","    avg_val_js_dist = my_js_dist(true_labels, predictions)\n","    print(\"  Accuracy (mse): \", avg_val_accuracy)\n","    print(\"mse: \", avg_val_mse)\n","    print(\"mae: \", avg_val_mae)\n","    print(\"js dist: \", avg_val_js_dist)\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur (mse).': avg_val_accuracy,\n","            'Valid. MSE.': avg_val_mse,\n","            'Valid. MAE.': avg_val_mae,\n","            'Valid. JS Dist.': avg_val_js_dist,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 2 ========\n","Training...\n","  Batch    40  of     44.    Elapsed: 0:00:09.\n","\n","  Average training loss: 0.08\n","  Training epcoh took: 0:00:10\n","\n","Running Validation...\n","  Accuracy (mse):  0.07439448693218302\n","mse:  0.07438176454568986\n","mae:  0.20682908089167945\n","js dist:  0.544336417402432\n","  Validation Loss: 0.07\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch    40  of     44.    Elapsed: 0:00:09.\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 0:00:10\n","\n","Running Validation...\n","  Accuracy (mse):  0.06582456337672318\n","mse:  0.06587576727356818\n","mae:  0.1924993417591356\n","js dist:  0.5056978510269803\n","  Validation Loss: 0.07\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tBrlg0NCColu","colab_type":"code","outputId":"e1fa90d0-1951-42a9-d8cc-2fbd2f38a5cd","executionInfo":{"status":"ok","timestamp":1591517053999,"user_tz":-480,"elapsed":38278,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# albert 0.07169099909003908\n","# distil 0.06872943142467139\n","# bert 0.06897136609064486\n","# roberta 0.06587576727356818\n","print(len(true_labels))\n","print(len(predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5\n","5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Unsjd9-LWlC8","colab_type":"code","outputId":"aae9fe16-2655-4abe-d9b6-055519d64de6","executionInfo":{"status":"ok","timestamp":1591517054000,"user_tz":-480,"elapsed":38273,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur (mse).</th>\n","      <th>Valid. MSE.</th>\n","      <th>Valid. MAE.</th>\n","      <th>Valid. JS Dist.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.08</td>\n","      <td>0.07</td>\n","      <td>0.07</td>\n","      <td>0.07</td>\n","      <td>0.19</td>\n","      <td>0.51</td>\n","      <td>0:00:10</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.07</td>\n","      <td>0.06</td>\n","      <td>0.06</td>\n","      <td>0.06</td>\n","      <td>0.19</td>\n","      <td>0.49</td>\n","      <td>0:00:10</td>\n","      <td>0:00:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  ...  Training Time  Validation Time\n","epoch                              ...                                \n","1               0.08         0.07  ...        0:00:10          0:00:00\n","2               0.07         0.06  ...        0:00:10          0:00:00\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"RQ6oblOk4wBl","colab_type":"code","outputId":"9c24b6bb-2aea-46a2-a74c-b22f2160ef2d","executionInfo":{"status":"ok","timestamp":1591517054809,"user_tz":-480,"elapsed":39074,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":481}},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"8dGc_ltBiB5N","colab_type":"text"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"KFmAd9VhhyXK","colab_type":"code","outputId":"86651b23-57fb-4a48-edef-9b3845b36e14","executionInfo":{"status":"ok","timestamp":1591463922807,"user_tz":-480,"elapsed":998,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","sentences = sentences_holdout\n","labels = labels_holdout\n","\n","# Report the number of sentences.\n","print('Number of test sentences:', sentences_holdout.shape)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of test sentences: (2,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hsOVCSYfnIyH","colab_type":"code","colab":{}},"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, \n","                      # token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p8Kjvpj2ArlD","colab_type":"code","colab":{}},"source":["print(true_labels[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwLnKZwAn60z","colab_type":"code","colab":{}},"source":["print(len(validation_dataloader))\n","print('MSE:', sklearn.metrics.mean_squared_error(np.concatenate(true_labels).ravel(), np.concatenate(predictions).ravel()))\n","print('MAE:', sklearn.metrics.mean_absolute_error(np.concatenate(true_labels).ravel(), np.concatenate(predictions).ravel()))\n","print('JS Dist:', my_js_dist(true_labels, predictions))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KO9S5zFafm0D","colab_type":"text"},"source":["# Save"]},{"cell_type":"code","metadata":{"id":"4kyT_66IWlDA","colab_type":"code","colab":{}},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './react_base_model/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0aUvvtMWlDE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8xqcA76bEyr","colab_type":"code","colab":{}},"source":["!cp -r ./react_base_model/ \"./drive/My Drive/Colab Notebooks/MainModels\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVg6j_t-blth","colab_type":"code","colab":{}},"source":["# # Load a trained model and vocabulary that you have fine-tuned\n","# model = model_class.from_pretrained(output_dir)\n","# tokenizer = tokenizer_class.from_pretrained(output_dir)\n","\n","# # Copy the model to the GPU.\n","# model.to(device)"],"execution_count":0,"outputs":[]}]}