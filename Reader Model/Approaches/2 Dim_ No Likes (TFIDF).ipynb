{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 Express: No Likes (TFIDF).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMx5b2QtcUNZoWotXRz6GvZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"c8B9sNNcwyBq","colab_type":"code","colab":{}},"source":["# pip install sklearn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_wbMW_7mmNu","colab_type":"code","outputId":"862c3408-9fde-4a68-ba11-d2c884ee19e7","executionInfo":{"status":"ok","timestamp":1591713719919,"user_tz":-480,"elapsed":2715,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yzN2-yCYkQnq","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"1-VbNYCEjJvo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","  # load your data using this function\n","  # url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ExpressData/2_No_Likes.csv'\n","  url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ExpressDataPre/EmoBank_Writer_All.csv'\n","  df = pd.read_csv(url, encoding='utf8')\n","  df = df[df['text'].apply(lambda x: isinstance(x, str))]\n","  \n","  data = df['text']\n","  labels = df[['V','A','D']]\n","\n","  data = data.values\n","  labels = labels.values\n","\n","  return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiG3Ed4kW02","colab_type":"text"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"D6Z_2bSpkcrF","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","\n","def create_model(input_dim):\n","  model = Sequential()\n","  model.add(Dense(units=500, activation='relu', input_dim=input_dim))\n","  model.add(Dense(units=3, activation='relu'))\n","  \n","  # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","  # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","  model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","  print(model.summary())\n","  from keras.utils.vis_utils import plot_model\n","  plot_model(model, to_file='2_Express_No_Likes_TFIDF.png', show_shapes=True, show_layer_names=True)\n","  # model.summary()\n","\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAGxM4t4kdFc","colab_type":"text"},"source":["# Train and Evaluate Model"]},{"cell_type":"code","metadata":{"id":"_RoJspSWkhhL","colab_type":"code","outputId":"151b81cd-c00c-448a-eed8-1262188ba16d","executionInfo":{"status":"ok","timestamp":1591713720391,"user_tz":-480,"elapsed":3168,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test):\n","  print(\"Training:\")\n","  # data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2, shuffle=True)\n","  data_test, data_val, labels_test, labels_val = train_test_split(data_test, labels_test, test_size=0.5, shuffle=True)\n","\n","  model.fit(data_train, labels_train, \n","        epochs=10, batch_size=128, verbose=1, shuffle=True,\n","        validation_data=(data_val, labels_val))\n","  \n","  print(\"Evaluating:\")\n","  scores = model.evaluate(data_test, labels_test, verbose=1)\n","  print(\"Final scores for fold:\")\n","  print(model.metrics_names, scores) \n","  return scores"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2G1rqoZ-lmIY","colab_type":"text"},"source":["# Run Evaluation"]},{"cell_type":"code","metadata":{"id":"pVL64lSAZ1KW","colab_type":"code","outputId":"61e2a074-2305-4ebf-8c9e-dee3ad780c37","executionInfo":{"status":"ok","timestamp":1591713720873,"user_tz":-480,"elapsed":3640,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data, labels = load_data()\n","print(len(data))\n","useHoldout = False\n","\n","min_reacts = 1\n","# if (len(data) > 10000):\n","#   useHoldout = True"],"execution_count":6,"outputs":[{"output_type":"stream","text":["10277\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8wEi4vnmlB6","colab_type":"code","outputId":"906dfca6-074f-4120-b520-4fdb07565e79","executionInfo":{"status":"ok","timestamp":1591713720874,"user_tz":-480,"elapsed":3632,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["print(labels)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[3.2  3.   3.4 ]\n"," [3.   2.67 3.  ]\n"," [2.8  3.4  2.8 ]\n"," ...\n"," [3.   2.8  2.8 ]\n"," [3.2  3.2  3.2 ]\n"," [2.6  3.6  2.6 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-ZN6gj0qTR4","colab_type":"text"},"source":["K-Fold"]},{"cell_type":"code","metadata":{"id":"jb4ElLALlpDB","colab_type":"code","outputId":"e71133e8-7bd2-48c9-9713-d5c024d23e91","executionInfo":{"status":"ok","timestamp":1591713722768,"user_tz":-480,"elapsed":5518,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import normalize\n","\n","\n","if not useHoldout:\n","  print(\"KFOLD\")  \n","  n_folds = 5\n","  kf = KFold(n_folds, shuffle=True)\n","  i = 0\n","\n","  # Define per-fold score containers\n","  scores_per_fold = []\n","\n","  for train_index, test_index in kf.split(data):\n","    print(\"Running Fold\", i+1, \"/\", n_folds)\n","    data_train, data_test = data[train_index], data[test_index]\n","    labels_train, labels_test = labels[train_index], labels[test_index]\n","\n","    #process\n","    vectorizer = TfidfVectorizer(max_features=5000)\n","    # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","    #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","    data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","    data_test = vectorizer.transform(data_test.astype('U'))\n","    # end\n","    \n","    model = None # Clearing the NN.\n","    model = create_model(len(vectorizer.get_feature_names()))\n","\n","    # scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","    # scores_per_fold.append(scores)\n","\n","    i += 1\n","\n","  "],"execution_count":8,"outputs":[{"output_type":"stream","text":["KFOLD\n","Running Fold 1 / 5\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 500)               2500500   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 1503      \n","=================================================================\n","Total params: 2,502,003\n","Trainable params: 2,502,003\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Running Fold 2 / 5\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_3 (Dense)              (None, 500)               2500500   \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 3)                 1503      \n","=================================================================\n","Total params: 2,502,003\n","Trainable params: 2,502,003\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Running Fold 3 / 5\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_5 (Dense)              (None, 500)               2500500   \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 3)                 1503      \n","=================================================================\n","Total params: 2,502,003\n","Trainable params: 2,502,003\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Running Fold 4 / 5\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 500)               2500500   \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 3)                 1503      \n","=================================================================\n","Total params: 2,502,003\n","Trainable params: 2,502,003\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Running Fold 5 / 5\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_9 (Dense)              (None, 500)               2500500   \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 3)                 1503      \n","=================================================================\n","Total params: 2,502,003\n","Trainable params: 2,502,003\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3fFIvAn4WA02","colab_type":"code","outputId":"f247e5a1-79e4-4f21-bbf4-b4beb8bf2844","executionInfo":{"status":"ok","timestamp":1591713722770,"user_tz":-480,"elapsed":5514,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["if not useHoldout:\n","\n","  print('Average scores across all folds:')\n","  for metric_index, metric_name in enumerate(metrics):\n","    metric_total = 0\n","    for scores in scores_per_fold:\n","      metric_total += scores[metric_index + 1]\n","    print(metric_name, metric_total/n_folds )\n","  print(scores_per_fold)\n","  "],"execution_count":9,"outputs":[{"output_type":"stream","text":["Average scores across all folds:\n","mean_squared_error 0.0\n","mean_absolute_error 0.0\n","<function js_distance at 0x7f8803f7b620> 0.0\n","[]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L8BF47pGqVpa","colab_type":"text"},"source":["Holdout"]},{"cell_type":"code","metadata":{"id":"UTR6zCWejJpJ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","if useHoldout:\n","  print(\"HOLDOUT\")\n","\n","  data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, shuffle=True)\n","\n","\n","  #process\n","  vectorizer = TfidfVectorizer(max_features=5000)\n","  # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","  #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","  data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","  data_test = vectorizer.transform(data_test.astype('U'))\n","  # end\n","\n","  model = None # Clearing the NN.\n","  model = create_model(len(vectorizer.get_feature_names()))\n","\n","  scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","  # print(model.metrics_names, scores) "],"execution_count":0,"outputs":[]}]}