{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 Facebook Word2Vec.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPSqiA+wTu1rGgGLVcYR/uM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"c8B9sNNcwyBq","colab_type":"code","outputId":"30c773cb-83f9-4907-a1e5-e3b371967b51","executionInfo":{"status":"ok","timestamp":1591714193608,"user_tz":-480,"elapsed":21563,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x_wbMW_7mmNu","colab_type":"code","outputId":"154f5e88-4c51-4f65-cf8d-d92e8434665c","executionInfo":{"status":"ok","timestamp":1591714195187,"user_tz":-480,"elapsed":23131,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yzN2-yCYkQnq","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"1-VbNYCEjJvo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","  # load your data using this function\n","  # url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCounts/2_No_Likes.csv'\n","  url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCountsPre/2_No_Likes.csv'\n","  df = pd.read_csv(url, encoding='utf16')\n","\n","  data = df['name']\n","  labels = df.select_dtypes(include=[np.number])\n","\n","  data = data.values\n","  labels = labels.values\n","\n","  return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiG3Ed4kW02","colab_type":"text"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"D6Z_2bSpkcrF","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, Embedding\n","from keras.utils import plot_model\n","\n","metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","\n","def create_model(embedding_layer):\n","  model = Sequential()\n","  model.add(embedding_layer)\n","  model.add(Dense(units=300, activation='relu'))\n","  model.add(Flatten())\n","  model.add(Dense(units=5, activation='relu'))\n","\n","  # model.add(Dense(300, input_dim=300, activation='relu'))\n","  # model.add(Dropout(0.2))\n","  # model.add(BatchNormalization())\n","\n","  # model.add(Dense(300, activation='relu'))\n","  # model.add(Dropout(0.3))\n","  # model.add(BatchNormalization())\n","\n","  \n","  # model.add(Conv1D(64, 5, activation='relu'))\n","  # model.add(MaxPooling1D(5))\n","  # model.add(Flatten())\n","  # model.add(Dense(units=64, activation='relu'))\n","  # model.add(Dense(units=5, activation='relu'))\n","  \n","  # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","  # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","  model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","  print(model.summary())\n","  from keras.utils.vis_utils import plot_model\n","  plot_model(model, to_file='4_FB.png', show_shapes=True, show_layer_names=True)\n","\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAGxM4t4kdFc","colab_type":"text"},"source":["# Train and Evaluate Model"]},{"cell_type":"code","metadata":{"id":"_RoJspSWkhhL","colab_type":"code","outputId":"54b3fdb5-bfda-47c3-9852-fa615da9d21f","executionInfo":{"status":"ok","timestamp":1591714196466,"user_tz":-480,"elapsed":24397,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test):\n","  print(\"Training:\")\n","  data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2, shuffle=True)\n","\n","  model.fit(data_train, labels_train, \n","        epochs=2, batch_size=128, verbose=1, shuffle=True,\n","        validation_data=(data_val, labels_val))\n","  \n","  print(\"Evaluating:\")\n","  scores = model.evaluate(data_test, labels_test, verbose=1)\n","  print(\"Final scores for fold:\")\n","  print(model.metrics_names, scores) \n","  return scores"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2G1rqoZ-lmIY","colab_type":"text"},"source":["# Run Evaluation"]},{"cell_type":"code","metadata":{"id":"pVL64lSAZ1KW","colab_type":"code","outputId":"0bd734e6-ffb7-4396-d289-c8144c361a4c","executionInfo":{"status":"ok","timestamp":1591714197937,"user_tz":-480,"elapsed":25858,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data, labels = load_data()\n","print(len(data))\n","useHoldout = False\n","\n","min_reacts = 1\n","if (len(data) > 10000):\n","  useHoldout = True"],"execution_count":6,"outputs":[{"output_type":"stream","text":["155696\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gyklDbWwzkUS","colab_type":"text"},"source":["Prep embeddings"]},{"cell_type":"code","metadata":{"id":"I8wEi4vnmlB6","colab_type":"code","outputId":"bcf4d3ed-9735-4586-b6f0-5daf9a806ad5","executionInfo":{"status":"ok","timestamp":1591714207539,"user_tz":-480,"elapsed":35451,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from gensim.models.keyedvectors import KeyedVectors\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding\n","\n","# path = './drive/My Drive/Colab Notebooks/Store/GoogleNews-vectors-negative300.bin.gz'\n","path = './drive/My Drive/Colab Notebooks/Store/fb_w2v_model.txt'\n","\n","# embedding_model = KeyedVectors.load_word2vec_format(path, binary=True)\n","# embedding_layer = embedding_model.wv.get_keras_embedding(train_embeddings=False)\n","\n","# vocabulary = {word: vector.index for word, vector in embedding_model.vocab.items()} \n","# tk = Tokenizer(num_words=len(vocabulary)) \n","# tk.word_index = vocabulary \n","# encoded_data = tk.texts_to_sequences(data)\n","# max_length = len(max(encoded_data, key=len))\n","# padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n","# data = padded_data\n","\n","embedding_dim = 100\n","t = Tokenizer()\n","t.fit_on_texts(data)\n","vocab_size = len(t.word_index) + 1\n","# integer encode the documents\n","encoded_data = t.texts_to_sequences(data)\n","max_length = len(max(encoded_data, key=len))\n","padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n","data = padded_data\n","\n","# MAX_NB_WORDS = 200000\n","word2vec = KeyedVectors.load_word2vec_format(path, binary=False)\n","\n","word_index = t.word_index\n","nb_words = min(len(word_index), len(word_index))+1\n","# nb_words = min(MAX_NB_WORDS, len(word_index))+1\n","\n","embedding_matrix = np.zeros((nb_words, embedding_dim))\n","for word, i in word_index.items():\n","    if word in word2vec.vocab:\n","        embedding_matrix[i] = word2vec.word_vec(word)\n","# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n","\n","embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n","                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=max_length,\n","                            trainable=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NU6yr2dd5N6a","colab_type":"code","outputId":"ccb61602-edbe-484b-f471-1bd009df5b26","executionInfo":{"status":"ok","timestamp":1591714207540,"user_tz":-480,"elapsed":35447,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(data.shape)\n","print(max_length)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(155696, 26)\n","26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-ZN6gj0qTR4","colab_type":"text"},"source":["K-Fold"]},{"cell_type":"code","metadata":{"id":"jb4ElLALlpDB","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import normalize\n","\n","\n","if not useHoldout:\n","  print(\"KFOLD\")  \n","  n_folds = 5\n","  kf = KFold(n_folds, shuffle=True)\n","  i = 0\n","\n","  # Define per-fold score containers\n","  scores_per_fold = []\n","\n","  for train_index, test_index in kf.split(data):\n","    print(\"Running Fold\", i+1, \"/\", n_folds)\n","    data_train, data_test = data[train_index], data[test_index]\n","    labels_train, labels_test = labels[train_index], labels[test_index]\n","\n","    labels_train_sums = labels_train.sum(axis = 1)\n","    has_min_reacts = labels_train_sums >= min_reacts\n","    data_train = data_train[has_min_reacts]\n","    labels_train = labels_train[has_min_reacts]\n","\n","    labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","    labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","\n","    #process\n","    # vectorizer = CountVectorizer(max_features=5000)\n","    # # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","    # #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","    # data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","    # data_test = vectorizer.transform(data_test.astype('U'))\n","    # end\n","\n","    \n","    \n","    model = None # Clearing the NN.\n","    model = create_model(embedding_layer)\n","\n","    scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","    scores_per_fold.append(scores)\n","\n","    i += 1\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fFIvAn4WA02","colab_type":"code","colab":{}},"source":["if not useHoldout:\n","\n","  print('Average scores across all folds:')\n","  for metric_index, metric_name in enumerate(metrics):\n","    metric_total = 0\n","    for scores in scores_per_fold:\n","      metric_total += scores[metric_index + 1]\n","    print(metric_name, metric_total/n_folds )\n","  print(scores_per_fold)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8BF47pGqVpa","colab_type":"text"},"source":["Holdout"]},{"cell_type":"code","metadata":{"id":"UTR6zCWejJpJ","colab_type":"code","outputId":"f5db0742-af56-46ed-8d26-10344f87d88b","executionInfo":{"status":"ok","timestamp":1591714230258,"user_tz":-480,"elapsed":58151,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":507}},"source":["from sklearn.model_selection import train_test_split\n","\n","if useHoldout:\n","  print(\"HOLDOUT\")\n","\n","  data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, shuffle=True)\n","\n","  labels_train_sums = labels_train.sum(axis = 1)\n","  has_min_reacts = labels_train_sums >= min_reacts\n","  data_train = data_train[has_min_reacts]\n","  labels_train = labels_train[has_min_reacts]\n","  \n","  labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","  labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","  #process\n","  # vectorizer = CountVectorizer(max_features=5000)\n","  # # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","  # #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","  # data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","  # data_test = vectorizer.transform(data_test.astype('U'))\n","  # end\n","\n","  # encoded_data_train = tk.texts_to_sequences(data_train)\n","  # padded_data_train = pad_sequences(encoded_data_train, maxlen=max_length, padding='post')\n","  # data_train = padded_data_train\n","\n","  # encoded_data_test = tk.texts_to_sequences(data_test)\n","  # padded_data_test = pad_sequences(encoded_data_test, maxlen=max_length, padding='post')\n","  # data_test = padded_data_test\n","\n","  model = None # Clearing the NN.\n","  model = create_model(embedding_layer)\n","\n","  scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","  # print(model.metrics_names, scores) "],"execution_count":11,"outputs":[{"output_type":"stream","text":["HOLDOUT\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 26, 100)           4157300   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 26, 300)           30300     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 7800)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 5)                 39005     \n","=================================================================\n","Total params: 4,226,605\n","Trainable params: 69,305\n","Non-trainable params: 4,157,300\n","_________________________________________________________________\n","None\n","Training:\n","Train on 99644 samples, validate on 24912 samples\n","Epoch 1/2\n","99644/99644 [==============================] - 10s 102us/step - loss: 0.0945 - mean_squared_error: 0.0945 - mean_absolute_error: 0.1851 - js_distance: 0.5274 - val_loss: 0.0868 - val_mean_squared_error: 0.0868 - val_mean_absolute_error: 0.1801 - val_js_distance: 0.5094\n","Epoch 2/2\n","99644/99644 [==============================] - 10s 97us/step - loss: 0.0839 - mean_squared_error: 0.0839 - mean_absolute_error: 0.1794 - js_distance: 0.5014 - val_loss: 0.0823 - val_mean_squared_error: 0.0823 - val_mean_absolute_error: 0.1782 - val_js_distance: 0.4966\n","Evaluating:\n","31140/31140 [==============================] - 2s 63us/step\n","Final scores for fold:\n","['loss', 'mean_squared_error', 'mean_absolute_error', 'js_distance'] [0.08226344926413161, 0.08226339519023895, 0.17809508740901947, 0.4964362382888794]\n"],"name":"stdout"}]}]}