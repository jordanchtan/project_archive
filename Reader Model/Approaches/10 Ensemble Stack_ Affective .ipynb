{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10 Ensemble Stack: Affective .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjgtfCNcUwjjs0jGSOm5nf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"q_fgeR-Nnls8","colab_type":"code","outputId":"ef53b04e-2155-4ec7-ed6a-f648126ea4ce","executionInfo":{"status":"ok","timestamp":1589699068335,"user_tz":-480,"elapsed":1580,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4RpOpKkVWCzE","colab_type":"code","colab":{}},"source":["import pandas as pd\n","url = \"https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/AffectiveText/affectivetext_test_normalized.csv\"\n","# url = \"https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/AffectiveTextPre/affectivetext_test_normalized.csv\"\n","df = pd.read_csv(url, encoding='utf8')\n","\n","sentences = df['message'].values\n","labels = df['reaction'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ow-buh5PwoOY","colab_type":"code","outputId":"33bc7f7a-fd24-470b-98d5-e0d3bd70afbc","executionInfo":{"status":"ok","timestamp":1589699072128,"user_tz":-480,"elapsed":5362,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":302}},"source":["!pip install transformers"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P_W-bDknxnu0","colab_type":"code","outputId":"7e234428-b1c6-4a98-d2b1-bd35b9b911d6","executionInfo":{"status":"ok","timestamp":1589699072129,"user_tz":-480,"elapsed":5356,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a5vjdimbeHoi","colab_type":"code","colab":{}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3PesWrQwJPG","colab_type":"code","colab":{}},"source":["from transformers import DistilBertPreTrainedModel, DistilBertModel\n","import torch.nn as nn\n","from transformers import BertTokenizer, DistilBertTokenizer\n","\n","\n","class MyDistilBertForSequenceClassificationReact(DistilBertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, 5)\n","        # self.classifier = nn.Linear(config.dim, config.num_labels)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n","        \n","        distilbert_output = self.distilbert(\n","            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n","        )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","        # new addition jordan\n","        logits = nn.ReLU()(logits)\n","        outputs = (logits,) + distilbert_output[1:]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1, 5), labels.view(-1, 5))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n","class MyDistilBertForSequenceClassificationExpress(DistilBertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, 3)\n","        # self.classifier = nn.Linear(config.dim, config.num_labels)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n","        \n","        distilbert_output = self.distilbert(\n","            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n","        )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","        # new addition jordan\n","        logits = nn.ReLU()(logits)\n","        outputs = (logits,) + distilbert_output[1:]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1, 3), labels.view(-1, 3))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAPAfQ8c6dRc","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","def predict_sentences(model, tokenizer, sentences):\n","  max_len = 36\n","\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in sentences:\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = max_len,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      input_ids.append(encoded_dict['input_ids'])\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  batch_size = 32\n","  prediction_data = TensorDataset(input_ids, attention_masks)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","  # Prediction on test set\n","  print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables \n","  predictions = []\n","  # Predict \n","  for batch in prediction_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    b_input_ids, b_input_mask = batch\n","    \n","    with torch.no_grad():\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask)\n","\n","    logits = outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","    predictions.append(logits)\n","  return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldv5ff0gtgB9","colab_type":"code","colab":{}},"source":["# import pandas as pd\n","# import numpy as np\n","\n","# # Load the dataset into a pandas dataframe.\n","# sentences_holdout = np.load(\"./drive/My Drive/Colab Notebooks/MainModels/sentences_holdout.npy\",allow_pickle=True)\n","# labels_holdout = np.load(\"./drive/My Drive/Colab Notebooks/MainModels/labels_holdout.npy\",allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5heef-m-T4VM","colab_type":"code","colab":{}},"source":["# print(sentences_holdout.shape)\n","# print(labels_holdout.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMcVvKiLDWB_","colab_type":"text"},"source":["Load express base model"]},{"cell_type":"code","metadata":{"id":"JxJsF-anDVFD","colab_type":"code","colab":{}},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/express_base_model\"\n","model = MyDistilBertForSequenceClassificationExpress.from_pretrained(\n","    output_dir, # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.cuda()\n","tokenizer = DistilBertTokenizer.from_pretrained(output_dir,do_lower_case=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tafSJ23Dhb1","colab_type":"text"},"source":["Express base model predictions"]},{"cell_type":"code","metadata":{"id":"cFNv66N2DuPK","colab_type":"code","outputId":"2c174808-873e-4630-84b6-90b7367f2a2c","executionInfo":{"status":"ok","timestamp":1589699075597,"user_tz":-480,"elapsed":8785,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["express_predictions = predict_sentences(model, tokenizer, sentences)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Predicting labels for 840 test sentences...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qYnvxcoCTkMq","colab_type":"code","outputId":"abc72599-afc1-4f78-f15d-d9d6058f9dc1","executionInfo":{"status":"ok","timestamp":1589699075598,"user_tz":-480,"elapsed":8779,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":571}},"source":["print(len(express_predictions))\n","print(express_predictions[0])"],"execution_count":34,"outputs":[{"output_type":"stream","text":["27\n","[[3.2002535 3.0154235 3.1236317]\n"," [2.4798014 3.1894133 3.0464993]\n"," [3.1983883 2.9973903 3.0309594]\n"," [3.274206  3.1051667 3.1556964]\n"," [2.2643116 3.202304  2.86701  ]\n"," [3.151666  3.1228383 3.0967882]\n"," [3.0537024 2.9844048 3.1118345]\n"," [2.9904726 2.9365482 3.1166155]\n"," [2.7489235 3.0442307 2.9661047]\n"," [2.9462383 2.9446058 3.0823064]\n"," [3.1305778 3.0579677 3.0316634]\n"," [2.985901  3.072596  3.191962 ]\n"," [3.1953058 3.183691  3.1172404]\n"," [2.9267178 2.9186056 3.0405316]\n"," [3.4037054 3.26381   3.2429748]\n"," [2.9579036 3.1233056 3.195844 ]\n"," [3.0429404 3.0380344 3.1899276]\n"," [2.1788538 3.1942031 2.8508978]\n"," [3.3466992 3.2805762 3.1490922]\n"," [2.9278162 2.8355424 3.0145154]\n"," [2.8877037 3.158996  3.242948 ]\n"," [2.5110693 3.1970377 3.173129 ]\n"," [2.915011  2.9948637 3.1189013]\n"," [3.3704855 3.144134  3.1737647]\n"," [3.2053916 2.9606693 3.1103935]\n"," [3.2126148 3.0696578 3.077311 ]\n"," [2.9808347 3.146453  3.097745 ]\n"," [2.4824417 3.2525227 3.115121 ]\n"," [3.0910156 2.927628  3.079396 ]\n"," [3.0222206 2.9433074 3.0787675]\n"," [2.9440439 3.178816  3.1759377]\n"," [3.2865872 3.1726496 3.217286 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7PROXfmbIyQZ","colab_type":"code","colab":{}},"source":["# np.save('./express_predictions.npy', express_predictions,allow_pickle=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNtz5AVWDZPN","colab_type":"text"},"source":["Load react base model\n"]},{"cell_type":"code","metadata":{"id":"CVQbzJucDftc","colab_type":"code","colab":{}},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/react_base_model\"\n","model = MyDistilBertForSequenceClassificationReact.from_pretrained(\n","    output_dir, # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.cuda()\n","tokenizer = DistilBertTokenizer.from_pretrained(output_dir,do_lower_case=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDJ6LIB9Duu-","colab_type":"text"},"source":["React base model predictions"]},{"cell_type":"code","metadata":{"id":"TxgDrspEZnzg","colab_type":"code","outputId":"5206bde6-b939-4524-f8e8-9fed2ce3e1a7","executionInfo":{"status":"ok","timestamp":1589699078913,"user_tz":-480,"elapsed":12079,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["react_predictions = predict_sentences(model, tokenizer, sentences)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Predicting labels for 840 test sentences...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RnAG82GVZoIE","colab_type":"text"},"source":["Combine"]},{"cell_type":"code","metadata":{"id":"bek_qcoOdzPu","colab_type":"code","outputId":"25dc2de0-7ae8-4aa6-e776-dab530492bda","executionInfo":{"status":"ok","timestamp":1589699078913,"user_tz":-480,"elapsed":12073,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import numpy as np\n","\n","react_predictions = np.vstack(react_predictions)\n","express_predictions = np.vstack(express_predictions)\n","print(react_predictions.shape)\n","print(express_predictions.shape)\n","data = np.concatenate((react_predictions, express_predictions),axis=1)\n","print(data.shape)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["(840, 5)\n","(840, 3)\n","(840, 8)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-1TOxALGdz7r","colab_type":"text"},"source":["Meta Learner"]},{"cell_type":"code","metadata":{"id":"IwAuZwCmUTHh","colab_type":"code","outputId":"be2654a6-cd4b-41df-a247-103fbd42e4a4","executionInfo":{"status":"ok","timestamp":1589699079618,"user_tz":-480,"elapsed":12771,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["from keras.models import load_model\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/\"\n","dependencies = {\n","    'js_distance': js_distance\n","}\n","model = load_model(output_dir + 'ensemble_meta_learner.h5', dependencies)\n","predictions = model.predict(data)\n","\n","def map_affect_reaction_to_index(reaction):\n","  reactions = {\"joy\": 0, \"surprise\": 1, \"sadness\": 2, \"anger\": 3}\n","  return reactions[reaction]\n","\n","def map_fb_idx_2_affect_idx(idx):\n","  # love_count,wow_count,haha_count,sad_count,angry_count\n","  if idx == 0 or idx == 2:\n","    # joy\n","    return 0\n","  elif idx == 1:\n","    # surprise\n","    return 1\n","  elif idx == 3:\n","    # sadness\n","    return 2\n","  elif idx == 4:\n","    # anger\n","    return 3\n","\n","\n","labels_idx = list(map(lambda react: map_affect_reaction_to_index(react), labels))\n","# print(labels_idx[4])\n","# print(predictions[4])\n","predictions_argmax = list(map(lambda pred: np.argmax(pred), predictions))\n","# print(predictions_argmax[4])\n","predictions_idx = list(map(lambda idx: map_fb_idx_2_affect_idx(idx), predictions_argmax))\n","\n","names = [\"joy\", \"surprise\", \"sadness\", \"anger\"]\n","print(sentences[800])\n","print(names)\n","print(labels_idx[800])\n","print(predictions_idx[800])\n","from sklearn import metrics\n","\n","print(metrics.classification_report(labels_idx, predictions_idx, target_names=names))\n","# ex border patrol agent beaten in prison\n","# ['joy', 'surprise', 'sadness', 'anger']\n","# 2\n","# 2\n","#               precision    recall  f1-score   support\n","\n","#          joy       0.60      0.87      0.71       362\n","#     surprise       0.34      0.16      0.22       184\n","#      sadness       0.81      0.50      0.61       202\n","#        anger       0.46      0.55      0.50        92\n","\n","#     accuracy                           0.59       840\n","#    macro avg       0.55      0.52      0.51       840\n","# weighted avg       0.58      0.59      0.56       840"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Ex-Border Patrol agent beaten in prison\n","['joy', 'surprise', 'sadness', 'anger']\n","2\n","3\n","              precision    recall  f1-score   support\n","\n","         joy       0.59      0.80      0.68       362\n","    surprise       0.37      0.27      0.31       184\n","     sadness       0.76      0.54      0.63       202\n","       anger       0.48      0.34      0.39        92\n","\n","    accuracy                           0.57       840\n","   macro avg       0.55      0.49      0.50       840\n","weighted avg       0.57      0.57      0.56       840\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VZR-cw_Oe5I1","colab_type":"text"},"source":["Manual test"]},{"cell_type":"code","metadata":{"id":"msrnlU3Xe3W_","colab_type":"code","colab":{}},"source":["affective_predictions = model.predict(data)"],"execution_count":0,"outputs":[]}]}