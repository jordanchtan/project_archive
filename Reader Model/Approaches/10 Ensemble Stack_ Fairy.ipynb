{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10 Ensemble Stack: Fairy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNjmc2GbLJclZN7uM4t/8Mx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"q_fgeR-Nnls8","colab_type":"code","outputId":"0f5c1069-19f0-4f30-aba1-499b8cafff98","executionInfo":{"status":"ok","timestamp":1589698525886,"user_tz":-480,"elapsed":25656,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4RpOpKkVWCzE","colab_type":"code","colab":{}},"source":["import pandas as pd\n","url = \"https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/FairyTales/fairy_tales_concat.csv\"\n","# url = \"https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/FairyTalesPre/fairy_tales_concat.csv\"\n","df = pd.read_csv(url, encoding='utf8')\n","\n","sentences = df['message'].values\n","labels = df['reaction'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ow-buh5PwoOY","colab_type":"code","outputId":"94574a5f-f40a-4f9f-d6de-24e954a55e1b","executionInfo":{"status":"ok","timestamp":1589698533184,"user_tz":-480,"elapsed":32919,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":554}},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n","\u001b[K     |████████████████████████████████| 645kB 4.7MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 13.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 51.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 52.1MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a867e6d391d85500317762ed874c98266cb60abf8d23bd856b05e974d2a60ea4\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P_W-bDknxnu0","colab_type":"code","outputId":"3ec72cce-f79f-49ae-91de-0643f107877d","executionInfo":{"status":"ok","timestamp":1589698536169,"user_tz":-480,"elapsed":35896,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a5vjdimbeHoi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"039be3f3-530b-419c-e97c-42c1db3b3edb","executionInfo":{"status":"ok","timestamp":1589698537743,"user_tz":-480,"elapsed":37467,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"F3PesWrQwJPG","colab_type":"code","colab":{}},"source":["from transformers import DistilBertPreTrainedModel, DistilBertModel\n","import torch.nn as nn\n","from transformers import BertTokenizer, DistilBertTokenizer\n","\n","\n","class MyDistilBertForSequenceClassificationReact(DistilBertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, 5)\n","        # self.classifier = nn.Linear(config.dim, config.num_labels)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n","        \n","        distilbert_output = self.distilbert(\n","            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n","        )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","        # new addition jordan\n","        logits = nn.ReLU()(logits)\n","        outputs = (logits,) + distilbert_output[1:]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1, 5), labels.view(-1, 5))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n","class MyDistilBertForSequenceClassificationExpress(DistilBertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.distilbert = DistilBertModel(config)\n","        self.pre_classifier = nn.Linear(config.dim, config.dim)\n","        self.classifier = nn.Linear(config.dim, 3)\n","        # self.classifier = nn.Linear(config.dim, config.num_labels)\n","        self.dropout = nn.Dropout(config.seq_classif_dropout)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n","        \n","        distilbert_output = self.distilbert(\n","            input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds\n","        )\n","        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n","        pooled_output = hidden_state[:, 0]  # (bs, dim)\n","        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n","        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n","        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n","        logits = self.classifier(pooled_output)  # (bs, dim)\n","        # new addition jordan\n","        logits = nn.ReLU()(logits)\n","        outputs = (logits,) + distilbert_output[1:]\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1, 3), labels.view(-1, 3))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAPAfQ8c6dRc","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","def predict_sentences(model, tokenizer, sentences):\n","  max_len = 36\n","\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in sentences:\n","      encoded_dict = tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          max_length = max_len,           # Pad & truncate all sentences.\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attn. masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      input_ids.append(encoded_dict['input_ids'])\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  batch_size = 32\n","  prediction_data = TensorDataset(input_ids, attention_masks)\n","  prediction_sampler = SequentialSampler(prediction_data)\n","  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","  # Prediction on test set\n","  print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables \n","  predictions = []\n","  # Predict \n","  for batch in prediction_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    b_input_ids, b_input_mask = batch\n","    \n","    with torch.no_grad():\n","        outputs = model(b_input_ids, \n","                        attention_mask=b_input_mask)\n","\n","    logits = outputs[0]\n","    logits = logits.detach().cpu().numpy()\n","    predictions.append(logits)\n","  return predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldv5ff0gtgB9","colab_type":"code","colab":{}},"source":["# import pandas as pd\n","# import numpy as np\n","\n","# # Load the dataset into a pandas dataframe.\n","# sentences_holdout = np.load(\"./drive/My Drive/Colab Notebooks/MainModels/sentences_holdout.npy\",allow_pickle=True)\n","# labels_holdout = np.load(\"./drive/My Drive/Colab Notebooks/MainModels/labels_holdout.npy\",allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5heef-m-T4VM","colab_type":"code","colab":{}},"source":["# print(sentences_holdout.shape)\n","# print(labels_holdout.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMcVvKiLDWB_","colab_type":"text"},"source":["Load express base model"]},{"cell_type":"code","metadata":{"id":"JxJsF-anDVFD","colab_type":"code","colab":{}},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/express_base_model\"\n","model = MyDistilBertForSequenceClassificationExpress.from_pretrained(\n","    output_dir, # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.cuda()\n","tokenizer = DistilBertTokenizer.from_pretrained(output_dir,do_lower_case=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tafSJ23Dhb1","colab_type":"text"},"source":["Express base model predictions"]},{"cell_type":"code","metadata":{"id":"cFNv66N2DuPK","colab_type":"code","outputId":"2b9810fc-eea8-457b-ef8f-6a85058b97bc","executionInfo":{"status":"ok","timestamp":1589698566403,"user_tz":-480,"elapsed":66110,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["express_predictions = predict_sentences(model, tokenizer, sentences)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Predicting labels for 5,108 test sentences...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qYnvxcoCTkMq","colab_type":"code","outputId":"2742e0b7-9d01-4ce1-f43f-afb3510b5102","executionInfo":{"status":"ok","timestamp":1589698566404,"user_tz":-480,"elapsed":66105,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":571}},"source":["print(len(express_predictions))\n","print(express_predictions[0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["160\n","[[3.1486962 3.024086  3.1031168]\n"," [3.0001125 3.04738   3.035521 ]\n"," [3.7224772 3.8547692 3.3431227]\n"," [2.7693386 3.5925481 3.2458398]\n"," [2.7586079 3.1759074 2.964889 ]\n"," [2.7639198 3.216053  3.1495366]\n"," [2.5587213 3.5013838 3.2401042]\n"," [3.3141787 3.8761125 3.4742594]\n"," [3.1055093 2.917144  3.071348 ]\n"," [2.7009838 3.648583  3.3501458]\n"," [2.918569  3.058998  3.1661115]\n"," [2.9796395 2.9538486 3.099547 ]\n"," [3.6779506 3.8171225 3.407195 ]\n"," [3.1801808 3.3245637 3.2315383]\n"," [3.4263954 3.8386765 3.3339663]\n"," [3.3142595 3.0813804 3.134345 ]\n"," [3.5757663 3.8822026 3.361874 ]\n"," [2.7894683 3.9272418 3.3861365]\n"," [2.953605  2.9339128 2.9852042]\n"," [2.6343734 3.376688  3.172567 ]\n"," [2.967202  3.0543604 3.1337838]\n"," [3.1454592 3.0081577 3.0744352]\n"," [3.1187432 3.4771683 3.2083728]\n"," [3.3415735 3.1637552 3.2101471]\n"," [3.4707997 3.3526049 3.2045567]\n"," [3.4871783 3.1073399 3.1439657]\n"," [3.071396  3.1042624 3.1191053]\n"," [2.8804653 3.1030862 2.9753323]\n"," [3.2155573 3.6443117 3.3879852]\n"," [3.3493323 3.2256942 3.1269493]\n"," [3.5792768 3.1562562 3.130865 ]\n"," [2.9974189 3.1021323 3.1048665]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7PROXfmbIyQZ","colab_type":"code","colab":{}},"source":["# np.save('./express_predictions.npy', express_predictions,allow_pickle=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNtz5AVWDZPN","colab_type":"text"},"source":["Load react base model\n"]},{"cell_type":"code","metadata":{"id":"CVQbzJucDftc","colab_type":"code","colab":{}},"source":["# Load a trained model and vocabulary that you have fine-tuned\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/react_base_model\"\n","model = MyDistilBertForSequenceClassificationReact.from_pretrained(\n","    output_dir, # Use the 12-layer BERT model, with an uncased vocab.\n","    # num_labels = 5, # The number of output labels--2 for binary classification.\n","    num_labels = 1, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.cuda()\n","tokenizer = DistilBertTokenizer.from_pretrained(output_dir,do_lower_case=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDJ6LIB9Duu-","colab_type":"text"},"source":["React base model predictions"]},{"cell_type":"code","metadata":{"id":"TxgDrspEZnzg","colab_type":"code","outputId":"0680fc67-9bdc-4f8e-cf67-1f2469f78cd4","executionInfo":{"status":"ok","timestamp":1589698584821,"user_tz":-480,"elapsed":84509,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["react_predictions = predict_sentences(model, tokenizer, sentences)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Predicting labels for 5,108 test sentences...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RnAG82GVZoIE","colab_type":"text"},"source":["Combine"]},{"cell_type":"code","metadata":{"id":"bek_qcoOdzPu","colab_type":"code","outputId":"793538a3-34a3-4b7b-a3b9-c34fa73052b5","executionInfo":{"status":"ok","timestamp":1589698584822,"user_tz":-480,"elapsed":84504,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import numpy as np\n","react_predictions = np.vstack(react_predictions)\n","express_predictions = np.vstack(express_predictions)\n","print(react_predictions.shape)\n","print(express_predictions.shape)\n","data = np.concatenate((react_predictions, express_predictions),axis=1)\n","print(data.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(5108, 5)\n","(5108, 3)\n","(5108, 8)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-1TOxALGdz7r","colab_type":"text"},"source":["Meta Learner"]},{"cell_type":"code","metadata":{"id":"IwAuZwCmUTHh","colab_type":"code","outputId":"d65ce7b2-6353-47d7-8735-a8aa284fb076","executionInfo":{"status":"ok","timestamp":1589698588800,"user_tz":-480,"elapsed":88476,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["from keras.models import load_model\n","output_dir = \"./drive/My Drive/Colab Notebooks/MainModels/\"\n","dependencies = {\n","    'js_distance': js_distance\n","}\n","model = load_model(output_dir + 'ensemble_meta_learner.h5', dependencies)\n","predictions = model.predict(data)\n","\n","def map_affect_reaction_to_index(reaction):\n","  reactions = {\"joy\": 0, \"surprise\": 1, \"sadness\": 2, \"anger\": 3}\n","  return reactions[reaction]\n","\n","def map_fb_idx_2_affect_idx(idx):\n","  # love_count,wow_count,haha_count,sad_count,angry_count\n","  if idx == 0 or idx == 2:\n","    # joy\n","    return 0\n","  elif idx == 1:\n","    # surprise\n","    return 1\n","  elif idx == 3:\n","    # sadness\n","    return 2\n","  elif idx == 4:\n","    # anger\n","    return 3\n","\n","\n","labels_idx = list(map(lambda react: map_affect_reaction_to_index(react), labels))\n","# print(labels_idx[4])\n","# print(predictions[4])\n","predictions_argmax = list(map(lambda pred: np.argmax(pred), predictions))\n","# print(predictions_argmax[4])\n","predictions_idx = list(map(lambda idx: map_fb_idx_2_affect_idx(idx), predictions_argmax))\n","\n","names = [\"joy\", \"surprise\", \"sadness\", \"anger\"]\n","print(sentences[800])\n","print(names)\n","print(labels_idx[800])\n","print(predictions_idx[800])\n","from sklearn import metrics\n","\n","print(metrics.classification_report(labels_idx, predictions_idx, target_names=names))\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The music sounded, and the people left the church hand-in-hand, with joy and gladness.\n","['joy', 'surprise', 'sadness', 'anger']\n","0\n","0\n","              precision    recall  f1-score   support\n","\n","         joy       0.48      0.87      0.62      1883\n","    surprise       0.37      0.15      0.22       945\n","     sadness       0.44      0.56      0.49       988\n","       anger       0.65      0.03      0.06      1292\n","\n","    accuracy                           0.47      5108\n","   macro avg       0.48      0.40      0.35      5108\n","weighted avg       0.50      0.47      0.38      5108\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VZR-cw_Oe5I1","colab_type":"text"},"source":["Manual test"]},{"cell_type":"code","metadata":{"id":"msrnlU3Xe3W_","colab_type":"code","colab":{}},"source":["affective_predictions = model.predict(data)"],"execution_count":0,"outputs":[]}]}