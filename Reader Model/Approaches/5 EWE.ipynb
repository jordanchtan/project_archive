{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5 EWE.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOn11Gbny6lfP+z4zxcleON"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"c8B9sNNcwyBq","colab_type":"code","outputId":"036f1c1a-d001-45bb-d20a-32555920c9f4","executionInfo":{"status":"ok","timestamp":1591716054062,"user_tz":-480,"elapsed":1558,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x_wbMW_7mmNu","colab_type":"code","outputId":"e0f97613-5b67-49b4-c39b-df6def8a92ef","executionInfo":{"status":"ok","timestamp":1591716055686,"user_tz":-480,"elapsed":3169,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yzN2-yCYkQnq","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"1-VbNYCEjJvo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","  # load your data using this function\n","  # url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCounts/2_No_Likes.csv'\n","  url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCountsPre/2_No_Likes.csv'\n","  df = pd.read_csv(url, encoding='utf16')\n","\n","  data = df['name']\n","  labels = df.select_dtypes(include=[np.number])\n","\n","  data = data.values\n","  labels = labels.values\n","\n","  return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiG3Ed4kW02","colab_type":"text"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"D6Z_2bSpkcrF","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, Embedding\n","from keras.utils import plot_model\n","\n","metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","\n","def create_model(embedding_layer):\n","  model = Sequential()\n","  model.add(embedding_layer)\n","  model.add(Dense(units=300, activation='relu'))\n","  model.add(Flatten())\n","  model.add(Dense(units=5, activation='relu'))\n","\n","  # model.add(Dense(300, input_dim=300, activation='relu'))\n","  # model.add(Dropout(0.2))\n","  # model.add(BatchNormalization())\n","\n","  # model.add(Dense(300, activation='relu'))\n","  # model.add(Dropout(0.3))\n","  # model.add(BatchNormalization())\n","\n","  \n","  # model.add(Conv1D(64, 5, activation='relu'))\n","  # model.add(MaxPooling1D(5))\n","  # model.add(Flatten())\n","  # model.add(Dense(units=64, activation='relu'))\n","  # model.add(Dense(units=5, activation='relu'))\n","  \n","  # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","  # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","  model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","  print(model.summary())\n","  from keras.utils.vis_utils import plot_model\n","  plot_model(model, to_file='5_EWE.png', show_shapes=True, show_layer_names=True)\n","\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAGxM4t4kdFc","colab_type":"text"},"source":["# Train and Evaluate Model"]},{"cell_type":"code","metadata":{"id":"_RoJspSWkhhL","colab_type":"code","outputId":"55847d34-6e56-4ae0-b33a-91efa37391fd","executionInfo":{"status":"ok","timestamp":1591716056133,"user_tz":-480,"elapsed":3574,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test):\n","  print(\"Training:\")\n","  data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2, shuffle=True)\n","\n","  model.fit(data_train, labels_train, \n","        epochs=2, batch_size=128, verbose=1, shuffle=True,\n","        validation_data=(data_val, labels_val))\n","  \n","  print(\"Evaluating:\")\n","  scores = model.evaluate(data_test, labels_test, verbose=1)\n","  print(\"Final scores for fold:\")\n","  print(model.metrics_names, scores) \n","  return scores"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2G1rqoZ-lmIY","colab_type":"text"},"source":["# Run Evaluation"]},{"cell_type":"code","metadata":{"id":"pVL64lSAZ1KW","colab_type":"code","outputId":"8c88054a-1f4d-4519-92ae-2a0216018e31","executionInfo":{"status":"ok","timestamp":1591716056535,"user_tz":-480,"elapsed":3966,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data, labels = load_data()\n","print(len(data))\n","useHoldout = False\n","\n","min_reacts = 1\n","# if (len(data) > 10000):\n","#   useHoldout = True"],"execution_count":6,"outputs":[{"output_type":"stream","text":["155696\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gyklDbWwzkUS","colab_type":"text"},"source":["Prep embeddings"]},{"cell_type":"code","metadata":{"id":"I8wEi4vnmlB6","colab_type":"code","outputId":"9052f62a-1de8-462a-b060-aec6023459e5","executionInfo":{"status":"ok","timestamp":1591716114676,"user_tz":-480,"elapsed":62091,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from gensim.models.keyedvectors import KeyedVectors\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding\n","\n","\n","path = './drive/My Drive/Colab Notebooks/Store/ewe_uni_w2v_model.txt'\n","\n","# embedding_model = KeyedVectors.load_word2vec_format(path, binary=True)\n","# embedding_layer = embedding_model.wv.get_keras_embedding(train_embeddings=False)\n","\n","# vocabulary = {word: vector.index for word, vector in embedding_model.vocab.items()} \n","# tk = Tokenizer(num_words=len(vocabulary)) \n","# tk.word_index = vocabulary \n","# encoded_data = tk.texts_to_sequences(data)\n","# max_length = len(max(encoded_data, key=len))\n","# padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n","# data = padded_data\n","\n","embedding_dim = 300\n","t = Tokenizer()\n","t.fit_on_texts(data)\n","vocab_size = len(t.word_index) + 1\n","# integer encode the documents\n","encoded_data = t.texts_to_sequences(data)\n","max_length = len(max(encoded_data, key=len))\n","padded_data = pad_sequences(encoded_data, maxlen=max_length, padding='post')\n","data = padded_data\n","\n","# MAX_NB_WORDS = 200000\n","word2vec = KeyedVectors.load_word2vec_format(path, binary=False)\n","\n","word_index = t.word_index\n","nb_words = min(len(word_index), len(word_index))+1\n","# nb_words = min(MAX_NB_WORDS, len(word_index))+1\n","\n","embedding_matrix = np.zeros((nb_words, embedding_dim))\n","for word, i in word_index.items():\n","    if word in word2vec.vocab:\n","        embedding_matrix[i] = word2vec.word_vec(word)\n","# print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n","\n","embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n","                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=max_length,\n","                            trainable=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NU6yr2dd5N6a","colab_type":"code","outputId":"78c5a613-ba74-46a1-f845-47f3d146f3fc","executionInfo":{"status":"ok","timestamp":1591716114677,"user_tz":-480,"elapsed":62081,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(data.shape)\n","print(max_length)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(155696, 26)\n","26\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-ZN6gj0qTR4","colab_type":"text"},"source":["K-Fold"]},{"cell_type":"code","metadata":{"id":"jb4ElLALlpDB","colab_type":"code","outputId":"1efbc0be-869e-4626-9f97-8e0efb741dbf","executionInfo":{"status":"error","timestamp":1591716152007,"user_tz":-480,"elapsed":99398,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import normalize\n","\n","if not useHoldout:\n","  print(\"KFOLD\")  \n","  n_folds = 5\n","  kf = KFold(n_folds, shuffle=True)\n","  i = 0\n","\n","  # Define per-fold score containers\n","  scores_per_fold = []\n","\n","  for train_index, test_index in kf.split(data):\n","    print(\"Running Fold\", i+1, \"/\", n_folds)\n","    data_train, data_test = data[train_index], data[test_index]\n","    labels_train, labels_test = labels[train_index], labels[test_index]\n","\n","    labels_train_sums = labels_train.sum(axis = 1)\n","    has_min_reacts = labels_train_sums >= min_reacts\n","    data_train = data_train[has_min_reacts]\n","    labels_train = labels_train[has_min_reacts]\n","\n","    labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","    labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","\n","    #process\n","    # vectorizer = CountVectorizer(max_features=5000)\n","    # # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","    # #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","    # data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","    # data_test = vectorizer.transform(data_test.astype('U'))\n","    # end\n","\n","    \n","    \n","    model = None # Clearing the NN.\n","    model = create_model(embedding_layer)\n","\n","    scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","    scores_per_fold.append(scores)\n","\n","    i += 1\n","\n","  "],"execution_count":9,"outputs":[{"output_type":"stream","text":["KFOLD\n","Running Fold 1 / 5\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 26, 300)           12471900  \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 26, 300)           90300     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 7800)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 5)                 39005     \n","=================================================================\n","Total params: 12,601,205\n","Trainable params: 129,305\n","Non-trainable params: 12,471,900\n","_________________________________________________________________\n","None\n","Training:\n","Train on 99644 samples, validate on 24912 samples\n","Epoch 1/2\n","99644/99644 [==============================] - 14s 144us/step - loss: 0.0699 - mean_squared_error: 0.0699 - mean_absolute_error: 0.1734 - js_distance: 0.4445 - val_loss: 0.0651 - val_mean_squared_error: 0.0651 - val_mean_absolute_error: 0.1693 - val_js_distance: 0.4321\n","Epoch 2/2\n","99644/99644 [==============================] - 14s 143us/step - loss: 0.0543 - mean_squared_error: 0.0543 - mean_absolute_error: 0.1595 - js_distance: 0.3982 - val_loss: 0.0542 - val_mean_squared_error: 0.0542 - val_mean_absolute_error: 0.1602 - val_js_distance: 0.3971\n","Evaluating:\n","31140/31140 [==============================] - 3s 80us/step\n","Final scores for fold:\n","['loss', 'mean_squared_error', 'mean_absolute_error', 'js_distance'] [0.054481296864797, 0.054481249302625656, 0.16037122905254364, 0.3963617980480194]\n","Running Fold 2 / 5\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 26, 300)           12471900  \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 26, 300)           90300     \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 7800)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 5)                 39005     \n","=================================================================\n","Total params: 12,601,205\n","Trainable params: 129,305\n","Non-trainable params: 12,471,900\n","_________________________________________________________________\n","None\n","Training:\n","Train on 99645 samples, validate on 24912 samples\n","Epoch 1/2\n","32896/99645 [========>.....................] - ETA: 8s - loss: 0.0944 - mean_squared_error: 0.0944 - mean_absolute_error: 0.1852 - js_distance: 0.5308"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-05cfb1ea3791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mscores_per_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-a8796ec7c5aa>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, data_train, labels_train, data_test, labels_test)\u001b[0m\n\u001b[1;32m     12\u001b[0m   model.fit(data_train, labels_train, \n\u001b[1;32m     13\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         validation_data=(data_val, labels_val))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"3fFIvAn4WA02","colab_type":"code","colab":{}},"source":["if not useHoldout:\n","\n","  print('Average scores across all folds:')\n","  for metric_index, metric_name in enumerate(metrics):\n","    metric_total = 0\n","    for scores in scores_per_fold:\n","      metric_total += scores[metric_index + 1]\n","    print(metric_name, metric_total/n_folds )\n","  print(scores_per_fold)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8BF47pGqVpa","colab_type":"text"},"source":["Holdout"]},{"cell_type":"code","metadata":{"id":"UTR6zCWejJpJ","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","if useHoldout:\n","  print(\"HOLDOUT\")\n","\n","  data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, shuffle=True)\n","\n","  labels_train_sums = labels_train.sum(axis = 1)\n","  has_min_reacts = labels_train_sums >= min_reacts\n","  data_train = data_train[has_min_reacts]\n","  labels_train = labels_train[has_min_reacts]\n","  \n","  labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","  labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","  #process\n","  # vectorizer = CountVectorizer(max_features=5000)\n","  # # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","  # #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","  # data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","  # data_test = vectorizer.transform(data_test.astype('U'))\n","  # end\n","\n","  # encoded_data_train = tk.texts_to_sequences(data_train)\n","  # padded_data_train = pad_sequences(encoded_data_train, maxlen=max_length, padding='post')\n","  # data_train = padded_data_train\n","\n","  # encoded_data_test = tk.texts_to_sequences(data_test)\n","  # padded_data_test = pad_sequences(encoded_data_test, maxlen=max_length, padding='post')\n","  # data_test = padded_data_test\n","\n","  model = None # Clearing the NN.\n","  model = create_model(embedding_layer)\n","\n","  # scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","  # print(model.metrics_names, scores) "],"execution_count":0,"outputs":[]}]}