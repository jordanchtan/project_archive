{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 No Likes (TFIDF).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpxXTQ9VvMlw+DMONff6Ao"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"c8B9sNNcwyBq","colab_type":"code","colab":{}},"source":["# Vectorization parameters\n","# Range (inclusive) of n-gram sizes for tokenizing text.\n","NGRAM_RANGE = (1, 2)\n","\n","# Limit on the number of features. We use the top 20K features.\n","TOP_K = 5000\n","\n","# Whether text should be split into word or character n-grams.\n","# One of 'word', 'char'.\n","TOKEN_MODE = 'word'\n","\n","# Minimum document/corpus frequency below which a token will be discarded.\n","MIN_DOCUMENT_FREQUENCY = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_wbMW_7mmNu","colab_type":"code","outputId":"f57407bb-d063-4a4b-f02c-e1e69b1f5ee3","executionInfo":{"status":"ok","timestamp":1591712694960,"user_tz":-480,"elapsed":3418,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras.backend as K\n","import tensorflow as tf\n","\n","from scipy.spatial.distance import jensenshannon\n","from numpy import asarray\n","\n","kl_div = tf.keras.losses.KLDivergence()\n"," \n","# calculate the js divergence\n","def js_divergence(p, q):\n","\tm = 0.5 * (p + q)\n","\treturn 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n","\n","def js_distance(y_true, y_pred):\n","  return K.sqrt(js_divergence(y_true, y_pred))\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yzN2-yCYkQnq","colab_type":"text"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"1-VbNYCEjJvo","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def load_data():\n","  # load your data using this function\n","  # url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCounts/2_No_Likes.csv'\n","  url = 'https://raw.githubusercontent.com/jordanchtan/EvaluationData/master/ReactDataCountsPre/2_No_Likes.csv'\n","  df = pd.read_csv(url, encoding='utf16')\n","\n","  data = df['name']\n","  labels = df.select_dtypes(include=[np.number])\n","\n","  data = data.values\n","  labels = labels.values\n","\n","  return data, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRiG3Ed4kW02","colab_type":"text"},"source":["# Create Model"]},{"cell_type":"code","metadata":{"id":"D6Z_2bSpkcrF","colab_type":"code","colab":{}},"source":["# from keras.models import Sequential\n","# from keras.layers import Dense\n","\n","# metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","\n","# def create_model(input_dim):\n","#   model = Sequential()\n","#   model.add(Dense(units=500, activation='relu', input_dim=input_dim))\n","#   model.add(Dense(units=5, activation='relu'))\n","  \n","#   # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","#   # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","#   model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","#   # model.summary()\n","\n","#   return model\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","\n","metrics = ['mean_squared_error', 'mean_absolute_error', js_distance]\n","\n","layers = 4\n","units = 128\n","dropout_rate = 0.2\n","def create_model(input_dim):\n","  model = Sequential()\n","  # model.add(Dropout(rate=dropout_rate, input_shape=(input_dim,1)))\n","  for x in range(layers - 1):\n","    model.add(Dense(units=units, activation='relu', input_dim=input_dim))\n","    model.add(Dropout(rate=dropout_rate))\n","  \n","  model.add(Dense(units=5, activation='relu'))\n","  \n","  \n","  # model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=metrics)\n","  # model.compile(loss=js_divergence, optimizer='adam', metrics=metrics)\n","  model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n","  \n","  print(model.summary())\n","  from keras.utils.vis_utils import plot_model\n","  plot_model(model, to_file='2_No_Likes_TFIDF.png', show_shapes=True, show_layer_names=True)\n","\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAGxM4t4kdFc","colab_type":"text"},"source":["# Train and Evaluate Model"]},{"cell_type":"code","metadata":{"id":"_RoJspSWkhhL","colab_type":"code","outputId":"4f8312a9-499e-4d1c-958e-da8bcaf21d0c","executionInfo":{"status":"ok","timestamp":1591712695408,"user_tz":-480,"elapsed":3834,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import nltk\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test):\n","  print(\"Training:\")\n","  data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2, shuffle=True)\n","  # data_test, data_val, labels_test, labels_val = train_test_split(data_test, labels_test, test_size=0.5, shuffle=True)\n","\n","  model.fit(data_train, labels_train, \n","        epochs=1, batch_size=128, verbose=1, shuffle=True,\n","        validation_data=(data_val, labels_val))\n","  \n","  print(\"Evaluating:\")\n","  scores = model.evaluate(data_test, labels_test, verbose=1)\n","  print(\"Final scores for fold:\")\n","  print(model.metrics_names, scores) \n","  return scores"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2G1rqoZ-lmIY","colab_type":"text"},"source":["# Run Evaluation"]},{"cell_type":"code","metadata":{"id":"pVL64lSAZ1KW","colab_type":"code","outputId":"6007c869-5687-4136-f0e4-05bd5a24522e","executionInfo":{"status":"ok","timestamp":1591712696594,"user_tz":-480,"elapsed":5004,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data, labels = load_data()\n","print(len(data))\n","useHoldout = False\n","\n","min_reacts = 1\n","if (len(data) > 10000):\n","  useHoldout = True"],"execution_count":6,"outputs":[{"output_type":"stream","text":["155696\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I8wEi4vnmlB6","colab_type":"code","outputId":"0d01cb6b-a680-495e-ba35-03956aca4046","executionInfo":{"status":"ok","timestamp":1591712696594,"user_tz":-480,"elapsed":4985,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["print(labels)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[  88    5    6   45    0]\n"," [ 109  186    1  499   44]\n"," [6634 5509 2854   19   10]\n"," ...\n"," [   2    0    1    0    0]\n"," [   0   11    1    0    0]\n"," [ 457 1109 3816   11   46]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A-ZN6gj0qTR4","colab_type":"text"},"source":["K-Fold"]},{"cell_type":"code","metadata":{"id":"jb4ElLALlpDB","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from sklearn.preprocessing import normalize\n","\n","\n","if not useHoldout:\n","  print(\"KFOLD\")  \n","  n_folds = 5\n","  kf = KFold(n_folds, shuffle=True)\n","  i = 0\n","\n","  # Define per-fold score containers\n","  scores_per_fold = []\n","\n","  for train_index, test_index in kf.split(data):\n","    print(\"Running Fold\", i+1, \"/\", n_folds)\n","    data_train, data_test = data[train_index], data[test_index]\n","    labels_train, labels_test = labels[train_index], labels[test_index]\n","\n","    labels_train_sums = labels_train.sum(axis = 1)\n","    has_min_reacts = labels_train_sums >= min_reacts\n","    data_train = data_train[has_min_reacts]\n","    labels_train = labels_train[has_min_reacts]\n","\n","    labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","    labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","\n","    #process\n","    vectorizer = TfidfVectorizer(max_features=5000)\n","    # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","    #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","    data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","    data_test = vectorizer.transform(data_test.astype('U'))\n","    # end\n","    \n","    model = None # Clearing the NN.\n","    model = create_model(len(vectorizer.get_feature_names()))\n","\n","    scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","    scores_per_fold.append(scores)\n","\n","    i += 1\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fFIvAn4WA02","colab_type":"code","colab":{}},"source":["if not useHoldout:\n","\n","  print('Average scores across all folds:')\n","  for metric_index, metric_name in enumerate(metrics):\n","    metric_total = 0\n","    for scores in scores_per_fold:\n","      metric_total += scores[metric_index + 1]\n","    print(metric_name, metric_total/n_folds )\n","  print(scores_per_fold)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8BF47pGqVpa","colab_type":"text"},"source":["Holdout"]},{"cell_type":"code","metadata":{"id":"UTR6zCWejJpJ","colab_type":"code","outputId":"b7df2bf5-6e38-40ef-f70e-ec93c10ba784","executionInfo":{"status":"ok","timestamp":1591712703624,"user_tz":-480,"elapsed":11978,"user":{"displayName":"Jordan Tan","photoUrl":"","userId":"08837459994650607663"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["from sklearn.model_selection import train_test_split\n","\n","if useHoldout:\n","  print(\"HOLDOUT\")\n","\n","  data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, shuffle=True)\n","\n","  labels_train_sums = labels_train.sum(axis = 1)\n","  has_min_reacts = labels_train_sums >= min_reacts\n","  data_train = data_train[has_min_reacts]\n","  labels_train = labels_train[has_min_reacts]\n","  \n","  labels_train = labels_train/labels_train.sum(axis=1, keepdims=True)\n","  labels_test = labels_test/labels_test.sum(axis=1, keepdims=True)\n","\n","  #process\n","  kwargs = {\n","        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n","        'dtype': 'int32',\n","        'strip_accents': 'unicode',\n","        'decode_error': 'replace',\n","        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n","        'min_df': MIN_DOCUMENT_FREQUENCY,\n","  }\n","  vectorizer = TfidfVectorizer(**kwargs)\n","  # vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n","  #                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\n","  data_train = vectorizer.fit_transform(data_train.astype('U'))\n","\n","  data_test = vectorizer.transform(data_test.astype('U'))\n","  # end\n","\n","  model = None # Clearing the NN.\n","  model = create_model(len(vectorizer.get_feature_names()))\n","\n","  # scores = train_and_evaluate_model(model, data_train, labels_train, data_test, labels_test)\n","\n","  #only 20k\n","# ['loss', 'mean_squared_error', 'mean_absolute_error', 'js_distance'] [0.049731985330294555, 0.04973198473453522, 0.15381042659282684, 0.3792986273765564]"],"execution_count":10,"outputs":[{"output_type":"stream","text":["HOLDOUT\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n","  UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 128)               18617856  \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 5)                 645       \n","=================================================================\n","Total params: 18,651,525\n","Trainable params: 18,651,525\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1RponkO4cjy3","colab_type":"code","colab":{}},"source":["# filename = './drive/My Drive/Colab Notebooks/express_model.h5'\n","# model.save(filename)"],"execution_count":0,"outputs":[]}]}